{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress MNIST to the smallest self extracting archive\n",
    "# gzip gets 9.63MB, or 166 bytes per sample\n",
    "# This is MDL, you add the bytes for the model, the bytes for the signal, and the bytes for the noise\n",
    "# And this is rather poorly done, ideally you want to build the model online\n",
    "\n",
    "\"\"\"\n",
    "compressing model.json      with len   11986 complen    1140\n",
    "compressing model.h5        with len  664504 complen  559453\n",
    "compressing signal.dat      with len 3840000 complen 2579712\n",
    "compressing noise.dat       with len 5882854 complen 5884667\n",
    "\"\"\"\n",
    "\n",
    "# Total size of x_train compressed is 9027925, about 6% better than gzip. Can you beat this?\n",
    "\n",
    "# See http://prize.hutter1.net for the inspiration.\n",
    "# I want to get at the question of what percent of data is noise.\n",
    "# Lossless compression is AI and is a true optimization metric you can compute.\n",
    "# Lossy compression is not good in the extremes and has fake loss functions.\n",
    "\n",
    "# As the world gets weird, it's important to get your optimization target exactly right.\n",
    "# Get it close, and we'll live in a hyperoptimized dystopia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "%pylab inline\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gzip\n",
    "from tqdm import trange\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train, y_test = [tf.keras.utils.to_categorical(x) for x in [y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST is 9.63MB gzipped, can we beat this with a net?\n",
    "# that's 166 bytes per sample\n",
    "x_train_gz = gzip.compress(x_train.data)\n",
    "len(x_train_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BITS = 64\n",
    "K.set_floatx('float32')\n",
    "\n",
    "def block(x_in, chan=32, strides=(2,2), bn=True, long=True):\n",
    "  x = Conv2D(chan, (3,3), padding='same', strides=strides)(x_in)\n",
    "  if bn:\n",
    "    x = BatchNormalization()(x)\n",
    "  if long:\n",
    "    x = ELU()(x)\n",
    "    x = Conv2D(chan, (3,3), padding='same')(x)\n",
    "    if bn:\n",
    "      x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    x_cut = Conv2D(chan, (1,1), padding='same', strides=strides)(x_in)\n",
    "    x = Add()([x, x_cut])\n",
    "  return ELU()(x)\n",
    "\n",
    "# encoder\n",
    "x = in1 = Input((28,28))\n",
    "x = Reshape((28,28,1))(x)\n",
    "x = Lambda(lambda x: (x-127.5)/127.5)(x)\n",
    "\n",
    "x = block(x, 64)\n",
    "x = block(x, 128)\n",
    "x = block(x, 192)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(BITS)(x)\n",
    "\n",
    "@tf.custom_gradient\n",
    "def binarize(x):\n",
    "  def grad(dy):\n",
    "    return dy\n",
    "  return tf.clip_by_value(tf.math.sign(x), 0, 1), grad\n",
    "\n",
    "#x = BatchNormalization()(x)\n",
    "#x = GaussianNoise(0.5)(x)\n",
    "#x = Dropout(rate=0.2)(x)\n",
    "#x = Lambda(lambda x: binarize(x), name=\"round\")(x)\n",
    "#x = Activation('tanh')(x)\n",
    "\n",
    "enc = Model(in1, x)\n",
    "enc.compile('sgd', 'mse')\n",
    "\n",
    "# decoder\n",
    "x = in1 = Input((BITS,))\n",
    "\n",
    "x = Dense(4*4*64)(x)\n",
    "x = ELU()(x)\n",
    "x = Reshape((4,4,64))(x)\n",
    "\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = block(x, 64, strides=(1,1))\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Cropping2D((1,1))(x)\n",
    "x = block(x, 64, strides=(1,1))\n",
    "\n",
    "x = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = ELU()(x)\n",
    "x = Conv2D(128, (1,1), padding='same')(x)\n",
    "x = ELU()(x)\n",
    "x = Conv2D(256, (1,1), padding='same')(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "# this is just a rescaling of sparse_categorical_crossentropy\n",
    "def loss(y_true, y_pred):\n",
    "  ll = tf.keras.backend.sparse_categorical_crossentropy(y_true, y_pred, False)\n",
    "  # (nats per input channel) * 784 / (log(2)*8) = 141.384\n",
    "  return (ll*784)/(np.log(2)*8)\n",
    "\n",
    "dec = Model(in1, x)\n",
    "dec.compile('sgd', loss)\n",
    "dec.summary()\n",
    "\n",
    "# autoencoder\n",
    "x = dec(enc.output)\n",
    "ae = Model(enc.inputs, x)\n",
    "ae.compile(Adam(3e-4), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit(x_train, x_train, validation_data=(x_test, x_test), batch_size=256, shuffle=True, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae.save_weights(\"/tmp/ae_model.h5\", overwrite=True)\n",
    "ae.load_weights(\"/tmp/ae_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data using the encoder\n",
    "z_swag = enc.predict(x_train, batch_size=256, verbose=1)\n",
    "z_swag = (z_swag*2).astype(np.int8)\n",
    "z_swag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this converts float32 into float16\n",
    "\n",
    "import h5py\n",
    "\n",
    "t1 = \"/tmp/weights32.h5\"\n",
    "t2 = \"/tmp/weights16.h5\"\n",
    "\n",
    "dec.save_weights(t1, overwrite=True)\n",
    "weights = h5py.File(t1)\n",
    "base_attrs = list(weights.attrs.items())\n",
    "weights16 = {}\n",
    "def print_attrs(name, obj):\n",
    "  if type(obj) == h5py._hl.dataset.Dataset:\n",
    "    weights16[name] = obj.value, list(obj.attrs.items())\n",
    "  else:\n",
    "    weights16[name] = None, list(obj.attrs.items())\n",
    "weights.visititems(print_attrs)\n",
    "weights.close()\n",
    "\n",
    "out = h5py.File(t2, \"w\")\n",
    "for k,v in base_attrs:\n",
    "  out.attrs.create(k,v)\n",
    "for name in weights16:\n",
    "  val, attrs = weights16[name]\n",
    "  if val is not None:\n",
    "    out[name] = val.astype(np.float16)\n",
    "  else:\n",
    "    out.create_group(name)\n",
    "  for k,v in attrs:\n",
    "    out[name].attrs.create(k,v)\n",
    "out.close()\n",
    "\n",
    "dec.load_weights(\"/tmp/weights16.h5\")\n",
    "print(os.path.getsize(t1), os.path.getsize(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of bytes required\n",
    "num_samples = 60000\n",
    "model_bytes = len(gzip.compress(dec.to_json().encode('utf-8')))\n",
    "model_bytes += len(gzip.compress(open(\"/tmp/weights16.h5\", \"rb\").read()))\n",
    "signal_bytes = len(gzip.compress(z_swag.data))\n",
    "noise_bytes = dec.evaluate(z_swag/2, x_train, batch_size=1024) * z_swag.shape[0]\n",
    "all_bytes = model_bytes + signal_bytes + noise_bytes\n",
    "\n",
    "print(\"%.2f model + %.2f sig + %.2f noise = %.2f\" %\n",
    "      tuple([x/num_samples for x in [model_bytes, signal_bytes, noise_bytes, all_bytes]]))\n",
    "print(\"total size is %f bytes\" % all_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth, argmax, entropy\n",
    "figsize(16,16)\n",
    "gt = np.concatenate(x_train[0:16], axis=1)\n",
    "coding_probs = dec.predict(z_swag[0:16]/2).reshape((-1,28,28,256))\n",
    "coding_probs = np.concatenate(coding_probs, axis=1)\n",
    "mu = np.argmax(coding_probs, axis=2)\n",
    "H = -np.sum(coding_probs * np.log2(coding_probs+1e-10), axis=-1)\n",
    "imshow(np.concatenate([gt, mu, H*16], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cpython cimport array\n",
    "import array\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from libc.stdint cimport uint64_t, int64_t, uint8_t\n",
    "\n",
    "cdef class Coder(object):\n",
    "  cdef uint64_t szmask,l,h,sz,val,precmult\n",
    "  cdef unsigned int prec,ptr,count,nbytes,lx,decode\n",
    "  cdef array.array ob\n",
    "  cdef uint64_t splits[257]\n",
    "  \n",
    "  def __init__(self, ob=[]):\n",
    "    self.ob = array.array('B', ob)\n",
    "    \n",
    "    self.nbytes = 6\n",
    "    self.sz = (self.nbytes-1)*8\n",
    "    self.szmask = (1L << self.sz) - 1\n",
    "    \n",
    "    # init these\n",
    "    self.ptr = 0\n",
    "    self.l = 0\n",
    "    self.h = (1L << (self.nbytes*8))-1\n",
    "    \n",
    "    # read in first bytes\n",
    "    if len(self.ob) > 0:\n",
    "      self.decode = 1\n",
    "      self.val = 0\n",
    "      for i in range(self.nbytes):\n",
    "        self.val = ((self.val & self.szmask) << 8) | self.ob[self.ptr]\n",
    "        self.ptr += 1\n",
    "    else:\n",
    "      self.decode = 0\n",
    "      self.val = -1\n",
    "    \n",
    "    # compute the splits for the probability distribution\n",
    "    # the precision doesn't have to be perfect, just the same\n",
    "    # note: 16 is too high here, the network isn't totally deterministic\n",
    "    self.prec = 14\n",
    "    self.precmult = ((1L<<self.prec) - 257)\n",
    "    \n",
    "    # number done so far\n",
    "    self.count = 0\n",
    "\n",
    "  def data(self):\n",
    "    return self.ob\n",
    "\n",
    "  cdef update_splits(self, np.ndarray[np.uint64_t, ndim=1] p):\n",
    "    cdef uint64_t crange = self.h - self.l\n",
    "\n",
    "    cdef int i\n",
    "    cdef uint64_t cp = 0\n",
    "    for i in range(0,257):\n",
    "      self.splits[i] = crange*cp\n",
    "      self.splits[i] >>= self.prec\n",
    "      self.splits[i] += self.l\n",
    "      \n",
    "      if i != 256:\n",
    "        # minimum is 1\n",
    "        cp += p[i] + 1\n",
    "      \n",
    "    # if any splits are the same, we messed up\n",
    "    # this bug isn't fixed, but increasing the bytes to 6 hides it\n",
    "    #if np.any((splits[1:] - splits[:-1]) == 0):\n",
    "    #  print(self.count, hex(self.l), hex(self.h), hex(self.h-self.l))\n",
    "    #  assert False\n",
    "      \n",
    "  cdef int search_splits(self):\n",
    "    cdef int i\n",
    "    for i in range(0,257):\n",
    "      if self.splits[i] > self.val:\n",
    "        #assert i != 0\n",
    "        return i-1\n",
    "    #assert False\n",
    "    return 256\n",
    "  \n",
    "  cdef push_bytes(self, decode=False):\n",
    "    while self.l>>self.sz == self.h>>self.sz:\n",
    "      #if self.l > self.h:\n",
    "      #  print(\"ISSUE\", hex(self.l), hex(self.h))\n",
    "      if self.decode:\n",
    "        #assert self.val >> self.sz == self.l >> self.sz\n",
    "        self.val = ((self.val & self.szmask) << 8) | self.ob[self.ptr]\n",
    "        self.ptr += 1\n",
    "      else:\n",
    "        #assert (self.l >> self.sz) < 256\n",
    "        array.resize_smart(self.ob, self.ptr+1)\n",
    "        self.ob[self.ptr] = self.l >> self.sz\n",
    "        self.ptr += 1\n",
    "        \n",
    "      self.l = ((self.l & self.szmask) << 8)\n",
    "      self.h = ((self.h & self.szmask) << 8) | 0xFF\n",
    "      \n",
    "  cdef void update(self, int x):\n",
    "    self.l, self.h = self.splits[x]+1, self.splits[x+1]\n",
    "    self.push_bytes()\n",
    "    #assert self.l < self.h\n",
    "      \n",
    "  def flush(self):\n",
    "    for i in range(self.nbytes):\n",
    "      self.ob.append(self.l >> self.sz)\n",
    "      self.l = ((self.l & self.szmask) << 8)\n",
    "      \n",
    "  def code(self, np.ndarray[np.float32_t, ndim=2] pf, np.ndarray[np.uint8_t, ndim=1] x):\n",
    "    cdef int n = pf.shape[0]\n",
    "    cdef int i\n",
    "    cdef uint64_t[256] p\n",
    "    \n",
    "    # discretize all\n",
    "    cdef np.ndarray[np.uint64_t, ndim=2] pn = (self.precmult*pf).astype(np.uint64)\n",
    "    \n",
    "    for i in range(0, n):\n",
    "      self.update_splits(pn[i])\n",
    "    \n",
    "      if self.decode:\n",
    "        x[i] = self.search_splits()\n",
    "\n",
    "      self.update(x[i])\n",
    "      self.count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Coder()\n",
    "DD = z_swag.shape[0]\n",
    "BS = 1024\n",
    "for i in range(0, DD, BS):\n",
    "  st = time.clock()\n",
    "  coding_probs = dec.predict(z_swag[i:i+BS]/2).reshape((-1,256))\n",
    "  c.code(coding_probs, x_train[i:i+BS].flatten())\n",
    "  #for p,x in zip(coding_probs, x_train[i:i+BS].flatten()):\n",
    "  #  c.code(p, x)\n",
    "  tlen = len(c.data())\n",
    "  et = time.clock() - st\n",
    "  print(\"%5d: total bytes: %7d  bytes per sample: %.3f  in %.2f ms\" % (i, tlen, tlen/(i+BS), et*1000.))\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tarfile, io\n",
    "\n",
    "def add_file(tt, nm, data):\n",
    "  gzdata = gzip.compress(data)\n",
    "  print(\"compressing %-15s with len %7d complen %7d\" % (nm, len(data), len(gzdata)))\n",
    "  tarinfo = tarfile.TarInfo(nm)\n",
    "  tarinfo.size = len(data)\n",
    "  tt.addfile(tarinfo, io.BytesIO(data))\n",
    "\n",
    "tt = tarfile.open(\"/tmp/mnist_compressed.gz\", \"w:gz\")\n",
    "add_file(tt, \"model.json\", dec.to_json().encode('utf-8'))\n",
    "add_file(tt, \"model.h5\", open(\"/tmp/weights16.h5\", \"rb\").read())\n",
    "add_file(tt, \"signal.dat\", z_swag.flatten().data)\n",
    "add_file(tt, \"noise.dat\", c.data())\n",
    "tt.close()\n",
    "\n",
    "os.path.getsize(\"/tmp/mnist_compressed.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** decompression starts here **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "%pylab inline\n",
    "\n",
    "# for verification\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "tt = tarfile.open(\"/tmp/mnist_compressed_2.gz\")\n",
    "model_json = tt.extractfile(\"model.json\").read()\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "with tempfile.NamedTemporaryFile() as tmpf:\n",
    "  tmpf.write(tt.extractfile(\"model.h5\").read())\n",
    "  model.load_weights(tmpf.name)\n",
    "  \n",
    "z_swag = np.frombuffer(tt.extractfile(\"signal.dat\").read(), dtype=np.uint8).reshape(-1, 64)\n",
    "noise = tt.extractfile(\"noise.dat\").read()\n",
    "\n",
    "tt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD = zswag.shape[0]\n",
    "BS = 2000\n",
    "\n",
    "dc = Coder(noise)\n",
    "for i in range(0, DD, BS):\n",
    "  st = time.clock()\n",
    "  coding_probs = model.predict(z_swag[i:i+BS]/2).reshape((-1,256))\n",
    "  outs = np.zeros((coding_probs.shape[0],), np.uint8)\n",
    "  dc.code(coding_probs, outs)\n",
    "  if np.all(x_train[i:i+BS].flatten() != np.array(outs)):\n",
    "    assert False\n",
    "  et = time.clock() - st\n",
    "  print(\"%5d: decoded successfully in %.2f ms\" % (i, et*1000.))\n",
    "\n",
    "figsize(8,8)\n",
    "imshow(np.array(outs).reshape(-1, 28, 28)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
